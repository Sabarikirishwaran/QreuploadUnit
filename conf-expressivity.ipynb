{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11149046,"sourceType":"datasetVersion","datasetId":6953311}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Model Definitions","metadata":{}},{"cell_type":"code","source":"!pip install -q numpy pandas torch matplotlib bluequbit pennylane pennylane-qiskit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T03:15:23.876095Z","iopub.execute_input":"2025-04-13T03:15:23.876394Z","iopub.status.idle":"2025-04-13T03:15:39.713697Z","shell.execute_reply.started":"2025-04-13T03:15:23.876373Z","shell.execute_reply":"2025-04-13T03:15:39.712871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reuploaded data on state color Map","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.animation import FuncAnimation\n\n# --------------------------------------\n# Define the QRU circuit\n# --------------------------------------\ndef QRU(params, x, alpha=0.5, reuploads=3):\n    for _ in range(reuploads):\n        for i in range(depth):\n            for j in range(len(x)):\n                qml.RY(x[j], wires=0)\n                qml.RX(params[i][2 * j], wires=0)\n                qml.RY(params[i][2 * j + 1], wires=0)\n\n# --------------------------------------\n# Parameters and dataset\n# --------------------------------------\nn_input = 10      # Sliding window size\ndepth = 3         # Circuit depth\nreuploads = 3     # Reuploads\nnp.random.seed(42)\ntime_series = np.random.rand(100)\n\n# Generate sliding windows\nwindows = [time_series[i:i+n_input] for i in range(len(time_series) - n_input + 1)]\n\n# Normalize each window\nnormalized_windows = [(w - np.min(w)) / (np.max(w) - np.min(w)) for w in windows]\n\n# --------------------------------------\n# Define Pennylane device and QNode\n# --------------------------------------\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qru_circuit(params, x):\n    QRU(params, x)\n    return qml.density_matrix(wires=0)\n\nparams = torch.tensor(np.full((depth, 2 * n_input), 0.5), requires_grad=False, dtype=torch.float64)\n\n# Compute final states for all windows\nfinal_states = [qru_circuit(params, w).detach().numpy() for w in normalized_windows]\n\n# --------------------------------------\n# Create Spherical Grid for Bloch Sphere\n# --------------------------------------\ntheta = np.linspace(0, 2 * np.pi, 60)   # azimuthal\nphi = np.linspace(0, np.pi, 60)         # polar\nTheta, Phi = np.meshgrid(theta, phi)\n\nX = np.sin(Phi) * np.cos(Theta)\nY = np.sin(Phi) * np.sin(Theta)\nZ = np.cos(Phi)\n\n# --------------------------------------\n# Plotting and Animation (with Time-Series)\n# --------------------------------------\nfig = plt.figure(figsize=(12, 10))\ngs = fig.add_gridspec(2, 1, height_ratios=[4, 1])\nax3d = fig.add_subplot(gs[0], projection='3d')  # Bloch sphere\nax2d = fig.add_subplot(gs[1])                   # Input time-series window\n\nsurf = [None]  # For holding the surface plot reference\n\ndef update(frame):\n    ax3d.clear()\n    ax2d.clear()\n\n    final_state = final_states[frame]\n    input_window = normalized_windows[frame]\n\n    # Compute amplitude-modulated radius R\n    R = np.zeros_like(Theta)\n    for j in range(Theta.shape[0]):\n        for k in range(Theta.shape[1]):\n            angle = Theta[j, k]\n            prob_amp = np.abs(\n                final_state[0, 0] * np.exp(1j * angle) +\n                final_state[1, 1] * np.exp(-1j * angle)\n            )\n            R[j, k] = prob_amp\n\n    # Spherical to deformed Cartesian coordinates\n    Xr = R * X\n    Yr = R * Y\n    Zr = R * Z\n\n    # Plot Bloch Sphere final state\n    surf[0] = ax3d.plot_surface(Xr, Yr, Zr, facecolors=cm.viridis(R / np.max(R)),\n                                rstride=1, cstride=1, antialiased=True, alpha=0.9)\n    ax3d.set_title(f\"QRU Final State on Bloch Sphere\\nTime Step: {frame}\", fontsize=14)\n    ax3d.set_xlabel('X (Real)')\n    ax3d.set_ylabel('Y (Imag)')\n    ax3d.set_zlabel('Z (Phase)')\n    ax3d.set_box_aspect([1,1,1])\n    ax3d.view_init(elev=30, azim=45)\n\n    # Plot Input Window below\n    ax2d.bar(range(len(input_window)), input_window, color='royalblue')\n    ax2d.set_ylim(0, 1)\n    ax2d.set_title(f\"Normalized Input Time-Series Window at t={frame}\", fontsize=12)\n    ax2d.set_xlabel(\"Time Step (within window)\")\n    ax2d.set_ylabel(\"Normalized Value\")\n\n# Create animation\nani = FuncAnimation(fig, update, frames=len(final_states), interval=300, repeat=True)\n\n# Show the animation\nplt.tight_layout()\nplt.show()\n\n# Optional: Save animation\nani.save(\"qru_parallel_visualization.mp4\", writer=\"ffmpeg\", fps=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T03:16:08.210932Z","iopub.execute_input":"2025-04-13T03:16:08.211254Z","iopub.status.idle":"2025-04-13T03:17:59.119516Z","shell.execute_reply.started":"2025-04-13T03:16:08.211225Z","shell.execute_reply":"2025-04-13T03:17:59.118907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#old code\nimport numpy as np\nimport torch\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the QRU circuit\ndef QRU(params, x, alpha=0.5, reuploads=3):\n    for _ in range(reuploads):\n        for i in range(depth):\n            for j in range(len(x)):\n                qml.RY(x[j], wires=0)\n                qml.RX(params[i][2 * j], wires=0)\n                qml.RY(params[i][2 * j + 1], wires=0)\n\n# Define the parameters and input data\nn_input = 10  # Window size\ndepth = 3  # Circuit depth\nreuploads = 3  # Number of reuploads\n\n# Generate a sample time series\nnp.random.seed(42)\ntime_series = np.random.rand(100)\n\n# Create a sliding window of size n_input\nwindow_size = n_input\nwindows = []\nfor i in range(len(time_series) - window_size + 1):\n    window = time_series[i:i + window_size]\n    windows.append(window)\n\n# Normalize the windows\nnormalized_windows = []\nfor window in windows:\n    normalized_window = (window - np.min(window)) / (np.max(window) - np.min(window))\n    normalized_windows.append(normalized_window)\n\n# Define the device\ndev = qml.device(\"default.qubit\", wires=1)\n\n# Define the QRU circuit with qnode\n@qml.qnode(dev)\ndef qru_circuit(params, x):\n    QRU(params, x)\n    return qml.density_matrix(wires=0)\n\n# Define the parameters\nparams = torch.tensor(np.full((depth, 2 * n_input), 0.5), requires_grad=False, dtype=torch.float64)\n\n# Run the QRU circuit on each window\nfinal_states = []\nfor window in normalized_windows:\n    state = qru_circuit(params, window)\n    final_states.append(state.detach().numpy())\n\n# Plot the final states as 3D maps\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Create a 3D grid of points\nx_grid = np.linspace(-1, 1, 100)\ny_grid = np.linspace(-1, 1, 100)\nX, Y = np.meshgrid(x_grid, y_grid)\n\n# Calculate the corresponding Z values for each reupload\nfor i, final_state in enumerate(final_states):\n    Z = np.zeros((100, 100))\n    for j in range(100):\n        for k in range(100):\n            # Calculate the probability amplitude at each point\n            prob_amp = np.abs(final_state[0, 0] * np.exp(1j * np.arctan2(Y[j, k], X[j, k])) + final_state[1, 1] * np.exp(-1j * np.arctan2(Y[j, k], X[j, k])))\n            Z[j, k] = prob_amp\n    ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.5)\n\n# Set labels and title\nax.set_xlabel('Real axis')\nax.set_ylabel('Imaginary axis')\nax.set_zlabel('Probability amplitude')\nax.set_title('Final states of the QRU circuit')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T12:55:45.489735Z","iopub.execute_input":"2025-04-09T12:55:45.490146Z","iopub.status.idle":"2025-04-09T13:00:41.515653Z","shell.execute_reply.started":"2025-04-09T12:55:45.490115Z","shell.execute_reply":"2025-04-09T13:00:41.514244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nfrom pennylane.qnn import TorchLayer\n\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\n\nn_input = 1\ndepth =3\n\n\n# ===== Definition of the quantum device =====\ndev = qml.device(\"default.qubit\", wires=1)\n\n# ===== Definition of the quantum circuits =====\n# QRU circuit with re-uploading\ndef QRU(params, x, alpha=0.5):\n    # The re-uploading circuit with 3 rotations per input per layer\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RY(x[j], wires=0)\n            qml.RX(params[i][2 * j], wires=0)\n            qml.RY(params[i][2 * j + 1], wires=0)\n            # qml.RZ(params[i][3 * j + 2], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n# Circuit without re-uploading (PQC)\ndef PQC(params, x):\n    # Data encoding in one shot\n    for j in range(len(x)):\n        qml.RY(x[j], wires=0)\n    # Followed by variational layers (here 2 rotations per input per layer)\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RX(params[i][2 * j], wires=0)\n            qml.RY(params[i][2 * j + 1], wires=0)            \n            # qml.RZ(params[i][3 * j + 2], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n# Definition of qnodes for each circuit\n@qml.qnode(dev, interface=\"torch\")\ndef circuit_qru(params, x):\n    return QRU(params, x)\n\n@qml.qnode(dev, interface=\"torch\")\ndef circuit_pqc(params, x):\n    return PQC(params, x)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:04.075199Z","iopub.execute_input":"2025-03-31T07:51:04.075652Z","iopub.status.idle":"2025-03-31T07:51:04.090178Z","shell.execute_reply.started":"2025-03-31T07:51:04.075615Z","shell.execute_reply":"2025-03-31T07:51:04.088949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inp_vqc = 1\ndepth_vqc = 8\nwindow_size = 3\n\n# ===== VQC with angle encoding and circular entanglement =====\ndef VQC(x, params):\n    # Angle encoding\n    qml.templates.AngleEmbedding(x, wires=range(inp_vqc), rotation=\"Y\")\n    \n    # Variational layers with RX and RY rotations + entanglement\n    for i in range(depth_vqc):\n        # Apply RX and RY rotations separately to use 2 parameters per qubit\n        for j in range(inp_vqc):\n            qml.RX(params[i][2 * j], wires=j)\n            #qml.RY(params[i][2 * j + 1], wires=j)\n        \n        # Circular entanglement layer (WITHOUT extra parameters)\n        entangler_params = params[i][inp_vqc:].reshape(1, inp_vqc)\n        qml.templates.BasicEntanglerLayers(entangler_params, wires=range(inp_vqc))\n    \n    return [qml.expval(qml.PauliZ(j)) for j in range(inp_vqc)]\n\n\nclass HybridVQC(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.fc = nn.Linear(inp_vqc, 1)  # FC layer: Maps inp_vqc → 1 output\n        self.fc.weight = nn.Parameter(self.fc.weight.double())  # Convert weights to float64\n        self.fc.bias = nn.Parameter(self.fc.bias.double())      # Convert bias to float64\n        \n        self.dev = qml.device(\"default.qubit\", wires=inp_vqc)\n        weight_shapes = {\"weights\": (depth_vqc, 2 * inp_vqc)}\n        @qml.qnode(self.dev, interface=\"torch\")\n        def qnode(inputs, weights):\n            return VQC(inputs, weights)\n        \n        # Wrap the QNode in a TorchLayer.\n        self.qlayer = TorchLayer(qnode, weight_shapes)\n\n    def forward(self, x):\n        q_output = self.qlayer(x)  # Already a torch tensor with autograd support        \n        q_output = q_output.view(1, -1)  # Reshape for FC layer (batch_size, inp_vqc)\n        return q_output\n        #return self.fc(q_output)  # Fully connected layer maps to a single output\n\n# Define the quantum device\nn_qbits = 1  # Data qubits\ncircuit_depth = 3\nn_ancillas = n_qbits * circuit_depth  # Multiple ancilla qubits\nnum_wires = n_qbits + n_ancillas  # Total number of wires\n\ndef quantum_circuit(inputs, weights):\n    for k in range(n_qbits):\n        for i in range(circuit_depth):\n            ancilla_wire = n_qbits + (k * circuit_depth) + i\n\n            # Apply controlled rotations using ancilla qubits\n            qml.Rot(weights[0][k][i][0], weights[0][k][i][1], weights[0][k][i][2], wires=ancilla_wire)\n            qml.Rot(weights[0][k][i][9], weights[0][k][i][10], weights[0][k][i][11], wires=k)\n            x =inputs\n            h =15\n            for j in range(x.size(0)):\n              qml.Rot(weights[0][k][i][h+j] * x[:,],weights[0][k][i][h+j+1] * x[:,],weights[0][k][i][h+j+2] * x[:,], wires=k)\n            qml.CRot(weights[0][k][i][3] * x[:,], weights[0][k][i][4] * x[:,], weights[0][k][i][5] * x[:,], wires=[ancilla_wire, k])\n            qml.Rot(weights[0][k][i][6], weights[0][k][i][7], weights[0][k][i][8], wires=k)\n            qml.Rot(weights[0][k][i][12], weights[0][k][i][13], weights[0][k][i][14], wires=ancilla_wire)\n\n    # Multi-ancilla measurement scheme\n    proj = (qml.Identity(wires=n_qbits) + qml.PauliZ(wires=n_qbits)) / 2\n    for m in range(n_qbits+1, num_wires):\n        proj = proj @ (qml.Identity(wires=m) + qml.PauliZ(wires=m)) / 2\n\n    dmeasure = qml.PauliZ(0)\n    for n in range(1, n_qbits):\n        dmeasure = dmeasure @ qml.PauliZ(n)\n\n    return qml.expval(dmeasure @ proj)\n\nclass QRB_global(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Quantum circuit weights (capturing frequency info)\n        # self.weights = nn.Parameter(\n        #     torch.rand((1, n_qbits, circuit_depth, 18), dtype=torch.float64, requires_grad=True)\n        # )\n        weight_shapes = {\"weights\": (1, n_qbits, circuit_depth, 18+12)}\n        self.bias_real = nn.Parameter(torch.tensor(1.0, dtype=torch.float64), requires_grad=True)\n        self.bias_imag = nn.Parameter(torch.tensor(1.0, dtype=torch.float64), requires_grad=True)\n\n        self.lnn = nn.Sequential(\n            nn.Linear(1, 1, dtype=torch.float64),\n            # nn.SiLU(),\n            # nn.Linear(128, 128, dtype=torch.float64),\n            # nn.SiLU(),\n            # nn.Linear(128, 1, dtype=torch.float64)\n        )\n        self.dev = qml.device(\"default.qubit\", wires=num_wires)\n        # Wrap the quantum circuit in a QNode and then in a TorchLayer.\n        @qml.qnode(self.dev, interface=\"torch\")\n        def qnode_circuit(inputs, weights):\n            return quantum_circuit(inputs, weights)\n\n        self.qlayer = TorchLayer(qnode_circuit, weight_shapes)\n\n    def forward(self, x):\n        # Get quantum circuit output (a real number capturing frequency info)\n        #qc_out = quantum_circuit(x, self.weights)\n        qc_out = []\n        for sample in x:\n            out = self.qlayer(sample)\n            qc_out.append(out)\n        qc_out = torch.stack(qc_out)\n        #qc_out = torch.transpose(qc_out, 0, 1)\n\n        # bias = self.bias_real + 1j * self.bias_imag\n        # out = qc_out + bias\n        # #out = out.abs()\n        # qc_out = out.abs()\n        # if isinstance(qc_out, torch.Tensor):\n        #     qc_out = qc_out.reshape(-1, window_size)  # reshape for Linear layer\n        qc_out = self.lnn(qc_out).squeeze(-1)\n\n        #return torch.real(qc_out)\n        return qc_out\n\n\ndev_qrbl = qml.device(\"default.qubit\", wires=2)\n\ndef qrb_local(params, x, alpha=0.5):\n    # Circuit avec re-uploading sur wire 0 et utilisation d'une seconde wire (wire 1)\n    qml.Hadamard(wires=1)\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RY(params[i][3 * j + 1] * x[j], wires=0)\n        qml.CRX(alpha * params[i][3 * len(x) + 1], wires=[1, 0])    \n        for j in range(len(x)):\n            qml.RX(params[i][3 * j], wires=0)\n            qml.RZ(params[i][3 * j + 2], wires=0)\n    qml.Hadamard(wires=1)\n    proj = (qml.Identity(wires=1) + qml.PauliZ(wires=1)) / 2\n    return qml.expval(qml.PauliZ(0) @ proj)\n\n@qml.qnode(dev_qrbl, interface=\"torch\")\ndef circuit_qrb_local(params, x):\n    return qrb_local(params, x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:04.902818Z","iopub.execute_input":"2025-03-31T07:51:04.903292Z","iopub.status.idle":"2025-03-31T07:51:04.935465Z","shell.execute_reply.started":"2025-03-31T07:51:04.903256Z","shell.execute_reply":"2025-03-31T07:51:04.934069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weights Loading and model initialization for inferencing","metadata":{}},{"cell_type":"code","source":"# ============================\n# ===== Load Models & Params\n# ============================\n\n# 1. Load trained params for QRU and PQC\nqru_params = torch.load(\"/kaggle/input/trained-models/trained_params_qru.pt\", weights_only=True)\npqc_params = torch.load(\"/kaggle/input/trained-models/trained_params_pqc.pt\", weights_only=True)\n\n# 2. Load trained HybridVQC model\nhybrid_vqc_model = HybridVQC()\nhybrid_vqc_model.load_state_dict(torch.load(\"/kaggle/input/trained-models/VQC_preTrained.pt\", weights_only=False))\n#hybrid_vqc_model.eval()\n\n\n#prep QRB_global_model\nqrb_global_model = QRB_global()\n\n#prep QRB_local_model\nparams_qrb_local = torch.tensor(np.full((depth, 3 * n_input + 2), 0.5),\n                              requires_grad=True, dtype=torch.float64)\n\n# ============================\n# Wrap all models into a callable list for Analysis\n# ============================\n\ndef eval_qru(x):\n    x= [x]\n    return circuit_qru(qru_params, x)\n\ndef eval_pqc(x):\n    x=[x]\n    return circuit_pqc(pqc_params, x)\n\ndef eval_vqc(x):    \n    x_tensor = torch.tensor(x, dtype=torch.float64).view(1, -1).clone().detach()\n    return hybrid_vqc_model(x_tensor).detach().numpy().flatten()[0]\n\nmodels = [eval_qru, eval_pqc, eval_vqc]\nmodel_names = [\"QRU–QRB-Local\", \"QRU–QRB-Global\", \"QRU\", \"PQC\", \"VQC\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:06.934781Z","iopub.execute_input":"2025-03-31T07:51:06.935291Z","iopub.status.idle":"2025-03-31T07:51:06.973584Z","shell.execute_reply.started":"2025-03-31T07:51:06.935249Z","shell.execute_reply":"2025-03-31T07:51:06.972215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data prep","metadata":{}},{"cell_type":"code","source":"total_points = 500  # Total number of simulated Mackey Glass points\ntrain_ratio = 0.8\n# ===== Generation of the Mackey Glass series =====\ndef generate_mackey_glass(total_steps, tau=17, n=10, beta=0.2, gamma=0.1, dt=1):\n    \"\"\"\n    Simulation of the Mackey Glass series using the Euler method.\n    The initial conditions (for t < tau) are set to 1.2.\n    \"\"\"\n    history = np.zeros(total_steps + tau)\n    history[:tau] = 1.2\n    for t in range(tau, total_steps + tau):\n        history[t] = history[t-1] + dt * (beta * history[t-tau] / (1 + history[t-tau]**n) - gamma * history[t-1])\n    return history[tau:]\n\nmackey_series = generate_mackey_glass(total_points)\n\n# ===== Normalization of the series between -1 and 1 =====\nmin_val = np.min(mackey_series)\nmax_val = np.max(mackey_series)\nmackey_series_norm = 2 * (mackey_series - min_val) / (max_val - min_val) - 1\n\n# ===== Creation of sliding windows =====\ndef create_windows(series, window_size):\n    X = []\n    Y = []\n    for i in range(len(series) - window_size):\n        X.append(series[i:i+window_size])\n        Y.append(series[i+window_size])\n    return np.array(X), np.array(Y)\n\nX, Y = create_windows(mackey_series_norm, n_input)\n\n# Conversion to torch.tensor (dtype=torch.float64 for consistency with PennyLane's device)\nX = torch.tensor(X, dtype=torch.float64)\nY = torch.tensor(Y, dtype=torch.float64)\n\n# Splitting into training and testing sets\nsplit_idx = int(len(X) * train_ratio)\nX_train, Y_train = X[:split_idx], Y[:split_idx]\nX_test, Y_test = X[split_idx:], Y[split_idx:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:07.876904Z","iopub.execute_input":"2025-03-31T07:51:07.877273Z","iopub.status.idle":"2025-03-31T07:51:07.966496Z","shell.execute_reply.started":"2025-03-31T07:51:07.877242Z","shell.execute_reply":"2025-03-31T07:51:07.965492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Complex Exponential Data","metadata":{}},{"cell_type":"code","source":"# np.random.seed(0)\n# torch.manual_seed(0)\n# train_ratio = 0.7\n\n# x_data = torch.linspace(-2 * np.pi, 2 * np.pi, 200, dtype=torch.float64)\n# frequencies = torch.tensor([0.0, 0.5, 1.0, 2.0, 2.5, 3.0], dtype=torch.float64)\n\n# # Pick a smaller common amplitude so we don't exceed ~ ±1 in the final sum.\n# common_amp = 1/6\n\n# # Choose some phases (all zeros here, except for the w=1 frequency which we offset by -pi/2\n# # to introduce a sine-like component). You can tweak these to see different shapes.\n# phases = torch.tensor([np.pi/2, 0, -np.pi/2, np.pi/4, np.pi/6, 0], dtype=torch.float64)\n\n# # Convert these phases into complex amplitudes with the same magnitude = common_amp\n# amplitudes = common_amp * torch.exp(1j * phases)\n\n# def target_function(x):\n#     \"\"\"\n#     y(x) = 2 * Re( sum_{k} [ a_k * exp(i * freq_k * x) ] )\n#     with equal amplitude for each frequency.\n#     \"\"\"\n#     y_complex = amplitudes[:, None] * torch.exp(1j * frequencies[:, None] * x)\n#     y = 2 * torch.real(torch.sum(y_complex, dim=0))\n#     return y\n\n# y_data = target_function(x_data)\n\n# # ===== Creation of sliding windows =====\n# def create_windows(series, window_size):\n#     X = []\n#     Y = []\n#     for i in range(len(series) - window_size):\n#         X.append(series[i:i+window_size])\n#         Y.append(series[i+window_size])\n#     return np.array(X), np.array(Y)\n\n# X, Y = create_windows(y_data, n_input)\n\n# # Conversion to torch.tensor (dtype=torch.float64 for consistency with PennyLane's device)\n# X = torch.tensor(X, dtype=torch.float64)\n# Y = torch.tensor(Y, dtype=torch.float64)\n\n# # Splitting into training and testing sets\n# split_idx = int(len(X) * train_ratio)\n# X_train, Y_train = X[:split_idx], Y[:split_idx]\n# X_test, Y_test = X[split_idx:], Y[split_idx:]\n\n# plt.plot(x_data.numpy(), y_data.numpy(), label=\"Sum of exponentials\")\n# plt.legend()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:08.902144Z","iopub.execute_input":"2025-03-31T07:51:08.902580Z","iopub.status.idle":"2025-03-31T07:51:08.907276Z","shell.execute_reply.started":"2025-03-31T07:51:08.902539Z","shell.execute_reply":"2025-03-31T07:51:08.906156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Expressivity Analysis","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# CONFIGURATION\n# -------------------------------\n# ========== EVALUATION UTILS ==========\n\ndef evaluate_model(model, inputs):   \n    with torch.no_grad():\n        output = model(inputs)  # ensure real input to quantum circuit\n    return np.array(output).flatten()\n\n# def plot_fft_magnitude(model_outputs, title):    \n#     plt.figure(figsize=(10, 6))\n#     for i, output in enumerate(model_outputs):  \n#         output = torch.tensor(np.array(output).reshape(-1), dtype=torch.float32) \n#         fft_vals = torch.fft.fft(output)        \n#         freqs = torch.fft.fftfreq(len(output))  # Hz                \n#         plt.stackplot(freqs, np.abs(fft_vals), label=f\"{model_names[i]}\")                \n#     plt.title(title)\n#     plt.xlabel(\"Frequency (Hz)\")\n#     plt.ylabel(\"Magnitude |FFT|\")\n#     plt.legend()\n#     plt.grid(True)\n#     plt.tight_layout()\n#     plt.show()\n\ndef plot_fft_magnitude_stackplot(model_outputs, title):\n    plt.figure(figsize=(10, 6))\n\n    fft_magnitudes = []\n    freqs = None\n\n    for i, output in enumerate(model_outputs):\n        output = torch.tensor(np.array(output).reshape(-1), dtype=torch.float32)\n        fft_vals = torch.fft.fft(output)\n        magnitude = torch.abs(fft_vals).numpy()\n\n        if freqs is None:\n            freqs = torch.fft.fftfreq(len(output)).numpy()\n\n        fft_magnitudes.append(magnitude)\n\n    # Convert to 2D array: shape (n_models, n_freqs)\n    fft_magnitudes = np.array(fft_magnitudes)\n\n    # Plot all at once\n    plt.stackplot(freqs, fft_magnitudes, labels=model_names)\n\n    plt.title(title)\n    plt.xlabel(\"Frequency (Hz)\")\n    plt.ylabel(\"Magnitude |FFT|\")\n    plt.legend(loc=\"upper right\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ndef count_significant_frequencies(model_outputs, threshold_ratio=0.1):\n    for i, output in enumerate(model_outputs):\n        fft_vals = np.fft.fft(output)\n        magnitude = np.abs(fft_vals)\n        max_magnitude = np.max(magnitude)\n        threshold = threshold_ratio * max_magnitude\n        count = np.sum(magnitude > threshold)\n        print(f\"Model: {model_names[i]} - Frequency count: {count}\")\n\n# ========== ANALYSIS 1 ==========\nprint(\"Running Analysis 1 (Trained Params - Expressivity)...\")\n\n# trained_outputs = []\n# for model in models:    \n#     outputs = [evaluate_model(model, x) for x in X_test]\n#     # truths = torch.tensor(np.array(Y_test).reshape(-1))\n#     # preds =  torch.tensor(np.array(outputs).reshape(-1))\n#     # acc = (torch.sum(torch.abs(preds-truths)<0.1))/len(truths)\n#     # print(\"acc: \", acc)\n#     trained_outputs.append(outputs)\n\n# plot_fft_magnitude_stackplot(trained_outputs, title=\"Analysis 1: |FFT| of Trained QML Outputs\")\n\n# count_significant_frequencies(trained_outputs, threshold_ratio=0.9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:09.975465Z","iopub.execute_input":"2025-03-31T07:51:09.975854Z","iopub.status.idle":"2025-03-31T07:51:09.986878Z","shell.execute_reply.started":"2025-03-31T07:51:09.975825Z","shell.execute_reply":"2025-03-31T07:51:09.985604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Mapping","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm  # fixed import\n\ntop_k_freqs = 100  # number of top significant frequencies to plot in Analysis 2\nn_repeats = 1000\n\ndef random_eval_qru(x):\n    rand_params = torch.rand_like(qru_params)    \n    return circuit_qru(rand_params, x)\n\ndef random_eval_pqc(x):\n    rand_params = torch.rand_like(pqc_params)    \n    return circuit_pqc(rand_params, x)\n\ndef random_eval_vqc(x):\n    # reinit weights in model\n    for param in hybrid_vqc_model.parameters():\n        param.data = torch.rand_like(param)\n    x_tensor = torch.tensor(x, dtype=torch.float64).view(1, -1)\n    return hybrid_vqc_model(x_tensor).detach().numpy().flatten()[0]\n\ndef random_eval_qrb_global(x):\n    # reinit weights in model\n    for param in qrb_global_model.parameters():\n        param.data = torch.rand_like(param)\n    x_tensor = torch.tensor(x, dtype=torch.float64).view(1, -1)\n    return qrb_global_model(x_tensor).detach().numpy().flatten()[0]\n\ndef random_eval_qrb_local(x):\n    rand_params = torch.rand_like(params_qrb_local)    \n    return circuit_qrb_local(rand_params, x)\n\nrandom_model_gens = [random_eval_qrb_local, random_eval_qrb_global, random_eval_qru, random_eval_pqc, random_eval_vqc]\n\nall_random_outputs = [[] for _ in range(len(random_model_gens))]\n\nfor i, model_gen in enumerate(random_model_gens):\n    print(f\"Sampling random weights for {model_names[i]}\")\n    for _ in tqdm(range(n_repeats), desc=f\"Model: {model_names[i]}\"):        \n        outputs = [evaluate_model(model_gen, x) for x in X_test]\n        all_random_outputs[i].append(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:51:18.437777Z","iopub.execute_input":"2025-03-31T07:51:18.438175Z","iopub.status.idle":"2025-03-31T09:22:48.865213Z","shell.execute_reply.started":"2025-03-31T07:51:18.438144Z","shell.execute_reply":"2025-03-31T09:22:48.863597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plotting","metadata":{}},{"cell_type":"code","source":"top_k = 3\n\n# ========== ANALYSIS 2 ==========\nprint(\"Running Analysis 2 (Random Params - Mapping Capability)...\")\n\ndef plot_all_complex_components(all_outputs, title):\n    fig, axs = plt.subplots(1, len(all_outputs), figsize=(18, 5))\n    \n    for i, model_outputs in enumerate(all_outputs):\n        for run_output in model_outputs:\n            run_output = torch.tensor(np.array(run_output).reshape(-1), dtype=torch.float32)\n            fft_vals = torch.fft.fft(run_output)\n            fft_mags = torch.abs(fft_vals)            \n            top_indices = torch.topk(fft_mags, top_k).indices\n            reals = torch.real(fft_vals)\n            imags = torch.imag(fft_vals)\n            axs[i].scatter(reals, imags, c='r', s=60, alpha=0.6)  # all runs on same axis\n\n        axs[i].set_title(f\"{model_names[i]}\")\n        axs[i].set_xlabel(\"Real\")\n        axs[i].set_ylabel(\"Imag\")\n        #axs[i].grid(True)\n\n    fig.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.savefig(\"aggregated_expressivity_fft.png\", dpi=300)\n    plt.show()\n\n# Plotting all outputs per model\nplot_all_complex_components(all_random_outputs, title=\"Expressivity Analysis 2: Re vs Im of FFT Coeffs over params\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:47:03.070283Z","iopub.execute_input":"2025-03-31T07:47:03.070594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Variance Measure","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef compute_fft_variance(all_outputs, model_names):\n    variances = {}\n    for model_idx, model_outputs in enumerate(all_outputs):\n        all_coeff_magnitudes = []\n\n        for run_output in model_outputs:\n            run_output = torch.tensor(np.array(run_output).reshape(-1), dtype=torch.float32)\n            fft_vals = torch.fft.fft(run_output)\n            fft_mags = torch.abs(fft_vals)\n            all_coeff_magnitudes.append(fft_mags.numpy())\n\n        all_coeff_magnitudes = np.array(all_coeff_magnitudes)\n        # Compute variance across random initializations at each frequency bin\n        freq_variance = np.var(all_coeff_magnitudes, axis=0)\n        # Compute mean variance across frequencies as a single metric\n        overall_variance = np.mean(freq_variance)\n\n        model_name = model_names[model_idx]\n        variances[model_name] = overall_variance\n\n    return variances\n\nfft_variance_result = compute_fft_variance(all_random_outputs, model_names)\nfor model, var in fft_variance_result.items():\n    print(f\"{model} FFT Coefficient Variance: {var:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:22:48.868590Z","iopub.execute_input":"2025-03-31T09:22:48.868926Z","iopub.status.idle":"2025-03-31T09:22:49.596357Z","shell.execute_reply.started":"2025-03-31T09:22:48.868899Z","shell.execute_reply":"2025-03-31T09:22:49.595405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ploting 2","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport numpy as np\n\ndef plot_top_k_complex_combined(all_outputs, model_names, top_k=3, title=\"Top-K FFT Coeffs\"):\n    colors = ['red', 'green', 'blue', 'violet', 'grey']  # one for each model\n    max_runs = max(len(runs) for runs in all_outputs)\n\n    plt.figure(figsize=(10, 8))\n\n    for model_idx, model_outputs in enumerate(np.flip(all_outputs)):\n        color = colors[model_idx]\n        for j, run_output in enumerate(model_outputs):\n            run_output = torch.tensor(np.array(run_output).reshape(-1), dtype=torch.float32)\n            fft_vals = torch.fft.fft(run_output)\n            fft_mags = torch.abs(fft_vals)\n\n            top_indices = torch.topk(fft_mags, top_k).indices\n            reals = torch.real(fft_vals)\n            imags = torch.imag(fft_vals)\n\n            # Opacity decreases with each run to show overlap\n            #alpha = 0.2 + 0.8 * (1 - j / max_runs)  # fade out later runs\n            alpha = (0.7 ** model_idx)\n            #alpha = 1.0\n            plt.scatter(reals, imags, c=color, s=30, alpha=alpha, label=model_names[model_idx] if j == 0 else None)\n\n    plt.title(title)\n    plt.xlabel(\"Real\")\n    plt.ylabel(\"Imag\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"top_k_fft_coeffs_combined.png\", dpi=300)\n    plt.show()\n\n# Plotting all outputs per model\nplot_top_k_complex_combined(all_random_outputs,model_names, title=\"Analysis 2: Re vs Im of FFT Coeffs over 10 Random Inits\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:43:59.728047Z","iopub.execute_input":"2025-03-31T13:43:59.728668Z","iopub.status.idle":"2025-03-31T13:47:58.432029Z","shell.execute_reply.started":"2025-03-31T13:43:59.728585Z","shell.execute_reply":"2025-03-31T13:47:58.430658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Quantum state distribution","metadata":{}},{"cell_type":"code","source":"import torch\nimport pennylane as qml\nimport pennylane.numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import entropy\nfrom tqdm import tqdm\n\n# ========== CONFIG ========== #\nn_qubits = 1\nn_input = 3\ndepth = 15\ninp_vqc = 3\ndepth_vqc = 15\nnum_samples = 1000\nnum_bins = 100\ncircuit_depth = depth\nn_ancillas = n_qubits * depth  # Multiple ancilla qubits\nnum_wires = n_qubits + n_ancillas  # Total number of wires\n# ========== DEVICES ========== #\ndev_qru = qml.device(\"default.qubit\", wires=n_qubits)\ndev_pqc = qml.device(\"default.qubit\", wires=n_qubits)\ndev_vqc = qml.device(\"default.qubit\", wires=inp_vqc)\ndev_qrb_local = qml.device(\"default.qubit\", wires=2)\ndev_global = qml.device(\"default.qubit\", wires=num_wires)\n\n# ========== CIRCUITS ========== #\ndef QRU(params, x, alpha=0.5):\n    # The re-uploading circuit with 3 rotations per input per layer\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RY(x[j], wires=0)\n            qml.RX(params[i][2 * j], wires=0)\n            qml.RY(params[i][2 * j + 1], wires=0)\n            \n    return qml.state()\n\n@qml.qnode(dev_qru, interface=\"torch\")\ndef circuit_qru(params, x):\n    return QRU(params, x)\n\ndef PQC(params, x):\n    for j in range(len(x)):\n        qml.RY(x[j], wires=0)\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RX(params[i][2 * j], wires=0)\n            qml.RY(params[i][2 * j + 1], wires=0)\n    return qml.state()\n\n@qml.qnode(dev_pqc, interface=\"torch\")\ndef circuit_pqc(params, x):\n    return PQC(params, x)\n\ndef VQC(params,x):\n    qml.templates.AngleEmbedding(x, wires=range(inp_vqc), rotation=\"Y\")\n    for i in range(depth_vqc):\n        for j in range(inp_vqc):\n            qml.RX(params[i][2 * j], wires=j)\n        entangler_params = params[i][inp_vqc:].reshape(1, inp_vqc)\n        qml.templates.BasicEntanglerLayers(entangler_params, wires=range(inp_vqc))\n    return qml.state()\n\n@qml.qnode(dev_vqc, interface=\"torch\")\ndef circuit_vqc(params, x):\n    return VQC(params,x)\n\n\ndef qrbGlobal_circuit(inputs, weights):\n    \n    for i in range(depth):\n        ancilla_wire = n_qubits + (k * circuit_depth) + i\n\n        # Apply controlled rotations using ancilla qubits\n        qml.Rot(weights[0][k][i][0], weights[0][k][i][1], weights[0][k][i][2], wires=ancilla_wire)\n        qml.Rot(weights[0][k][i][9], weights[0][k][i][10], weights[0][k][i][11], wires=k)\n        x =inputs\n        h =15\n        for j in range(x.size(0)):\n          qml.Rot(weights[0][k][i][h+j] * x[:,],weights[0][k][i][h+j+1] * x[:,],weights[0][k][i][h+j+2] * x[:,], wires=k)\n        qml.CRot(weights[0][k][i][3] * x[:,], weights[0][k][i][4] * x[:,], weights[0][k][i][5] * x[:,], wires=[ancilla_wire, k])\n        qml.Rot(weights[0][k][i][6], weights[0][k][i][7], weights[0][k][i][8], wires=k)\n        qml.Rot(weights[0][k][i][12], weights[0][k][i][13], weights[0][k][i][14], wires=ancilla_wire)\n\n    # # Multi-ancilla measurement scheme\n    # proj = (qml.Identity(wires=n_qbits) + qml.PauliZ(wires=n_qbits)) / 2\n    # for m in range(n_qbits+1, num_wires):\n    #     proj = proj @ (qml.Identity(wires=m) + qml.PauliZ(wires=m)) / 2\n\n    # dmeasure = qml.PauliZ(0)\n    # for n in range(1, n_qbits):\n    #     dmeasure = dmeasure @ qml.PauliZ(n)\n\n    return qml.state()\n\nclass qrbGlobal(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        weight_shapes = {\"weights\": (1, n_qbits, circuit_depth, 18+12)}\n        self.bias_real = nn.Parameter(torch.tensor(1.0, dtype=torch.float64), requires_grad=True)\n        self.bias_imag = nn.Parameter(torch.tensor(1.0, dtype=torch.float64), requires_grad=True)\n\n        self.lnn = nn.Sequential(nn.Linear(1, 1, dtype=torch.float64))\n        self.dev = qml.device(\"default.qubit\", wires=num_wires)\n        \n        # Wrap the quantum circuit in a QNode and then in a TorchLayer.\n        @qml.qnode(self.dev, interface=\"torch\")\n        def qnode_circuit(inputs, weights):\n            return qrbGlobal_circuit(inputs, weights)\n\n        self.qlayer = TorchLayer(qnode_circuit, weight_shapes)\n\n    def forward(self, x):        \n        pass\n\nqrb_global_model = QRB_global()\ndef qrb_circuit_global(params, x):\n    with torch.no_grad():\n        x_tensor = torch.tensor(x, dtype=torch.float64).reshape(1, -1)\n        qrb_global_model.qlayer.qnode.construct([x_tensor[0], params], {})  # set weights\n        state = qrb_global_model.qlayer.qnode(x_tensor[0], params)\n        return state\n\ndef qrbLocal(params, x, alpha=0.5):\n    # Circuit avec re-uploading sur wire 0 et utilisation d'une seconde wire (wire 1)\n    qml.Hadamard(wires=1)\n    for i in range(depth):\n        for j in range(len(x)):\n            qml.RY(params[i][3 * j + 1] * x[j], wires=0)\n        qml.CRX(alpha * params[i][3 * len(x) + 1], wires=[1, 0])    \n        for j in range(len(x)):\n            qml.RX(params[i][3 * j], wires=0)\n            qml.RZ(params[i][3 * j + 2], wires=0)\n    qml.Hadamard(wires=1)\n    proj = (qml.Identity(wires=1) + qml.PauliZ(wires=1)) / 2\n    return qml.state()\n\n@qml.qnode(dev_qrb_local, interface=\"torch\")\ndef circuitQrb_local(params, x):\n    return qrbLocal(params, x)\n\n# ========== METRIC COMPUTATION ========== #\ndef state_fidelity(psi1, psi2, model_name):\n    if \"global\" in model_name:\n        return qml.math.fidelity(psi1, psi2, check_state=False)\n    else:\n        return qml.math.fidelity(psi1, psi2, check_state=True)\n\ndef haar_distribution(N, num_bins=100):\n    f_vals = np.linspace(0, 1, 10000)\n    p_vals = (N - 1) * (1 - f_vals) ** (N - 2)\n    p_vals /= np.trapz(p_vals, f_vals)  # Normalize by area, not sum\n    hist, bin_edges = np.histogram(f_vals, bins=num_bins, weights=p_vals, density=True)\n    return hist, bin_edges\n\ndef sample_haar_reference_distribution(num_samples=1000, num_bins=100):\n    fidelities = []\n    for _ in range(num_samples):\n        U1 = sample_random_unitary()\n        U2 = sample_random_unitary()\n        psi1 = U1 @ np.array([1.0, 0.0])\n        psi2 = U2 @ np.array([1.0, 0.0])\n        psi1 =qml.math.dm_from_state_vector(psi1)\n        psi2 =qml.math.dm_from_state_vector(psi2)\n        fid = state_fidelity(psi1, psi2, \"haar\")\n        fidelities.append(fid)\n    \n    hist, bin_edges = np.histogram(fidelities, bins=num_bins, range=(0, 1), density=True)\n    return hist, bin_edges\n\ndef sample_random_unitary():\n    # Generate a Haar-random unitary from U(2)\n    Z = np.random.randn(2, 2) + 1j * np.random.randn(2, 2)\n    Q, R = np.linalg.qr(Z)\n    D = np.diag(R) / np.abs(np.diag(R))\n    return Q @ np.diag(D)\n\n\ndef kl_expressibility(qnode, param_shape, x_input, n_wires, model_name, fixed_haar=None):\n    fidelities = []\n    for _ in tqdm(range(num_samples), desc=f\"Sampling for {model_name}\"):\n        θ1 = torch.rand(param_shape, dtype=torch.float64)\n        θ2 = torch.rand(param_shape, dtype=torch.float64)\n\n        psi1 = qnode(θ1, x_input).detach().numpy()\n        psi1 =qml.math.dm_from_state_vector(psi1)\n        psi2 = qnode(θ2, x_input).detach().numpy()\n        psi2 =qml.math.dm_from_state_vector(psi2)\n\n        fid = state_fidelity(psi1, psi2, str(model_name))\n        fidelities.append(fid)\n\n    fidelities = np.array(fidelities)\n    pf_hist, bins = np.histogram(fidelities, bins=num_bins, range=(0, 1), density=True)\n\n    ph_hist = fixed_haar\n    \n    # # Use sample-based Haar distribution for 1-qubit models\n    # if model_name in [\"QRU\", \"PQC\"] and n_wires == 1:\n    #     ph_hist = fixed_haar\n    # else:\n    #     ph_hist, _ = haar_distribution(2 ** n_wires, num_bins)    \n\n    eps = 1e-10\n    pf_hist += eps\n    ph_hist += eps\n\n    kl = entropy(pf_hist, ph_hist)\n    return kl, pf_hist, ph_hist, bins\n\n# def kl_expressibility(qnode, param_shape, X_inputs, n_wires, model_name, num_bins=100):\n#     fidelities = []\n#     for _ in tqdm(range(num_samples), desc=f\"Sampling for {model_name}\"):\n#         θ1 = torch.rand(param_shape, dtype=torch.float64)\n#         θ2 = torch.rand(param_shape, dtype=torch.float64)\n\n#         # Loop through multiple inputs\n#         for x in X_inputs:\n#             psi1 = qnode(θ1, x).detach().numpy()\n#             psi2 = qnode(θ2, x).detach().numpy()\n\n#             fid = state_fidelity(psi1, psi2)\n#             fidelities.append(fid)\n\n#     fidelities = np.array(fidelities)\n#     pf_hist, bins = np.histogram(fidelities, bins=num_bins, range=(0, 1), density=True)\n#     ph_hist, _ = haar_distribution(2 ** n_wires, num_bins)\n\n#     eps = 1e-10\n#     pf_hist += eps\n#     ph_hist += eps\n\n#     kl = entropy(pf_hist, ph_hist)\n#     return kl, pf_hist, ph_hist, bins\n\n\n# ========== PLOT ========== #\ndef plot_distributions(pf, ph, bins, name):\n    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n    plt.figure(figsize=(8, 5))\n    plt.plot(bin_centers, pf, label=f\"{name} Fidelity Dist.\", lw=2)\n    plt.plot(bin_centers, ph, label=\"Haar Dist.\", linestyle=\"--\", lw=2)\n    plt.xlabel(\"Fidelity\")\n    plt.ylabel(\"Probability\")\n    plt.title(f\"Expressibility: {name}\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"expressibility_{name.lower()}.png\", dpi=300)\n    plt.show()\n\ndef run_kl_trials(qnode, param_shape, x_input, n_wires, model_name, fixed_haar, n_trials=5):\n    kl_vals = []\n    for i in range(n_trials):\n        print(f\"\\nTrial {i+1}/{n_trials} for {model_name}\")\n        kl, _, _, _ = kl_expressibility(qnode, param_shape, x_input, n_wires, model_name, fixed_haar=fixed_haar)\n        kl_vals.append(kl)\n    kl_avg = np.mean(kl_vals)\n    kl_std = np.std(kl_vals)\n    print(f\"\\nAverage KL Divergence for {model_name} over {n_trials} trials: {kl_avg:.5f} ± {kl_std:.5f}\")\n    return kl_avg, kl_std\n\n# ========== RUN ========== #\nif __name__ == \"__main__\":\n    x_fixed = torch.tensor([0.1], dtype=torch.float64)\n    #x_fixed = X_test\n\n    fixed_haar, _ = sample_haar_reference_distribution(num_samples, num_bins)\n    analytic_haar_vqc, _ = haar_distribution(2 ** inp_vqc, num_bins)\n    analytic_haar_qrbg, _ = haar_distribution(2 ** num_wires, num_bins)\n\n    # print(\"\\n--- Running trials for KL Estimation ---\")\n    # run_kl_trials(circuit_qru, (depth, 2 * n_input), x_fixed, n_qubits, \"QRU\", fixed_haar)\n    # run_kl_trials(circuit_pqc, (depth, 2 * n_input), x_fixed, n_qubits, \"PQC\", fixed_haar)\n    # run_kl_trials(circuit_vqc, (depth_vqc, 2 * inp_vqc), x_fixed, inp_vqc, \"HybridVQC\", analytic_haar)    \n    #run_kl_trials(circuit_qrb_local, (depth, 3 * n_input + 2), x_fixed, 2, \"QRU–QRB-Local\", fixed_haar)        \n    #run_kl_trials(circuit_qrb_global, (1, n_qubits, depth, 30), x_fixed, num_wires, \"QRU–QRB-Global\", fixed_haar)\n\n    # QRB_global\n    param_shape_qrbg = (1, n_qubits, depth, 30)\n    kl_qrbg, pf_qrbg, ph_qrbg, bins_qrbg = kl_expressibility(qrb_circuit_global, param_shape_qrbg, x_fixed, num_wires, \"QRU-QRB-global\", fixed_haar=analytic_haar_qrbg)\n    plot_distributions( pf_qrbg, ph_qrbg, bins_qrbg, \"QRU-QRB-global\")\n    \n    # QRB_local\n    param_shape_qrbl = (depth_vqc, 2 * inp_vqc)\n    kl_qrbl, pf_qrbl, ph_qrbl, bins_qrbl = kl_expressibility(circuitQrb_local, param_shape_qrbl, x_fixed, 2, \"QRU-QRB-local\", fixed_haar=fixed_haar)\n    plot_distributions(pf_qrbl, ph_qrbl, bins_qrbl, \"QRU-QRB-local\")\n    \n    # QRU\n    param_shape_qru = (depth, 2 * n_input)\n    kl_qru, pf_qru, ph_qru, bins_qru = kl_expressibility(circuit_qru, param_shape_qru, x_fixed, n_qubits, \"QRU\", fixed_haar=fixed_haar)\n    plot_distributions(pf_qru, ph_qru, bins_qru, \"QRU\")\n\n    # PQC\n    param_shape_pqc = (depth, 2 * n_input)\n    kl_pqc, pf_pqc, ph_pqc, bins_pqc = kl_expressibility(circuit_pqc, param_shape_pqc, x_fixed, n_qubits, \"PQC\", fixed_haar=fixed_haar)\n    plot_distributions(pf_pqc, ph_pqc, bins_pqc, \"PQC\")\n\n    # Hybrid VQC\n    param_shape_vqc = (depth_vqc, 2 * inp_vqc)\n    kl_vqc, pf_vqc, ph_vqc, bins_vqc = kl_expressibility(circuit_vqc, param_shape_vqc, x_fixed, inp_vqc, \"VQC\", fixed_haar=analytic_haar_vqc)\n    plot_distributions(pf_vqc, ph_vqc, bins_vqc, \"VQC\")\n\n    \n    # Print all KLs\n    print(\"\\nKL Divergences:\")\n    print(f\"QRU: {kl_qru:.5f}\")\n    print(f\"PQC: {kl_pqc:.5f}\")\n    print(f\"VQC: {kl_vqc:.5f}\")\n    print(f\"QRB_local: {kl_qrbl:.5f}\")\n    print(f\"QRB_global: {kl_qrbg:.5f}\")\n","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-03-31T10:25:27.037723Z","iopub.execute_input":"2025-03-31T10:25:27.039293Z","iopub.status.idle":"2025-03-31T12:14:09.625596Z","shell.execute_reply.started":"2025-03-31T10:25:27.039218Z","shell.execute_reply":"2025-03-31T12:14:09.624313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}